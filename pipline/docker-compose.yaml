version: '3.8'

x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE:-custom-airflow:latest}  # Will use your built image
  build:
    context: .
    dockerfile: Dockerfile.airflow
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
    - AIRFLOW__CORE__FERNET_KEY=''  # Generate a real one for prod!
    - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
    - _PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-apache-spark  # Latest provider
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
  user: "0:0"  # Root for init, but you can switch to airflow uid later
  networks:
    - smartcity-network

services:
  # ============================================================================
  # HADOOP HDFS
  # ============================================================================
  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - "9870:9870"
      - "8020:8020"
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    env_file:
      - ./hadoop.env
    volumes:
      - namenode-data:/opt/hadoop/dfs/name
    networks:
      - smartcity-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode:
    image: apache/hadoop:3.3.6
    container_name: datanode
    hostname: datanode
    command: ["hdfs", "datanode"]
    env_file:
      - ./hadoop.env
    volumes:
      - datanode-data:/opt/hadoop/dfs/data
    networks:
      - smartcity-network
    depends_on:
      namenode:
        condition: service_healthy

  # ============================================================================
  # KAFKA (single node KRaft)
  # ============================================================================
  kafka:
    image: apache/kafka:latest
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
      CLUSTER_ID: "Mk3OEYBSD34fcwNTJENDM2Qk"  # Random fixed ID
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - smartcity-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions.sh --bootstrap-server localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5

  # ============================================================================
  # SPARK Standalone
  # ============================================================================
  spark-master:
    image: apache/spark:latest
    container_name: spark-master
    hostname: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master --host spark-master --port 7077 --webui-port 8080
    ports:
      - "8080:8080"  # Master UI
      - "7077:7077"
    environment:
      - SPARK_MODE=master
    volumes:
      - ./spark-jobs:/opt/spark-jobs
      - ./data:/opt/data
    networks:
      - smartcity-network
    depends_on:
      - namenode

  spark-worker:
    image: apache/spark:latest
    container_name: spark-worker
    hostname: spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    volumes:
      - ./spark-jobs:/opt/spark-jobs
      - ./data:/opt/data
    networks:
      - smartcity-network
    depends_on:
      - spark-master

  # ============================================================================
  # AIRFLOW
  # ============================================================================
  postgres-airflow:
    image: postgres:16
    container_name: postgres-airflow
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres-airflow-data:/var/lib/postgresql/data
    networks:
      - smartcity-network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5

  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow db migrate
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || true
    depends_on:
      postgres-airflow:
        condition: service_healthy

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8082:8080"
    depends_on:
      - postgres-airflow
      - airflow-init
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    depends_on:
      - postgres-airflow
      - airflow-init

  # ============================================================================
  # POSTGRESQL (app DB)
  # ============================================================================
  postgres:
    image: postgres:16
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=smartcity
      - POSTGRES_PASSWORD=smartcity
      - POSTGRES_DB=smartcity
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - smartcity-network

  # ============================================================================
  # REDIS
  # ============================================================================
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"  # Changed to standard port
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    networks:
      - smartcity-network

  # ============================================================================
  # PROMETHEUS & GRAFANA
  # ============================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - smartcity-network

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - smartcity-network

  # ============================================================================
  # ADMINER
  # ============================================================================
  adminer:
    image: adminer:latest
    container_name: adminer
    ports:
      - "8083:8080"
    environment:
      - ADMINER_DEFAULT_SERVER=postgres
    depends_on:
      - postgres
    networks:
      - smartcity-network

volumes:
  kafka-data:
  postgres-data:
  postgres-airflow-data:
  redis-data:
  grafana-data:
  prometheus-data:
  namenode-data:
  datanode-data:

networks:
  smartcity-network:
    driver: bridge