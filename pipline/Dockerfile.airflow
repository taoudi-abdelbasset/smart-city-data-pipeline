FROM apache/airflow:2.8.1

USER root

# Install Java 17 (required for Spark) and curl
RUN apt-get update && \
    apt-get install -y openjdk-17-jdk curl && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Install Hadoop client
ENV HADOOP_VERSION=3.3.6
ENV HADOOP_HOME=/opt/hadoop
RUN curl -L https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz | \
    tar -xz -C /opt && \
    mv /opt/hadoop-${HADOOP_VERSION} ${HADOOP_HOME}

# Set environment variables
ENV PATH="${HADOOP_HOME}/bin:${PATH}"
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

USER airflow

# Install Python packages for Spark and HDFS
RUN pip install --no-cache-dir \
    apache-airflow-providers-apache-spark==4.5.0 \
    pyspark==3.5.0 \
    hdfs3