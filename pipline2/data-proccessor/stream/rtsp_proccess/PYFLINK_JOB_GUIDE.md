# PyFlink Vision Job - Complete Guide

## What Changed? ğŸ”„

### Old Code (kafka-python)
```python
from kafka import KafkaConsumer  # âŒ Not using Flink
consumer = KafkaConsumer(...)
for message in consumer:
    process(message)
```
- âŒ Won't show in Flink UI
- âŒ Not using Flink's engine
- âŒ No parallelization
- âŒ No fault tolerance

### New Code (PyFlink)
```python
from pyflink.datastream import StreamExecutionEnvironment  # âœ… Using Flink!
env = StreamExecutionEnvironment.get_execution_environment()
stream = env.from_source(kafka_source)
stream.map(process).print()
env.execute("Vision Job")  # âœ… Submits to Flink
```
- âœ… **Shows in Flink UI** ğŸ¯
- âœ… Uses Flink's distributed engine
- âœ… Automatic parallelization
- âœ… Fault tolerance with checkpointing
- âœ… Managed state

## Files You Need

### 1. Place in `data-processor/stream/rtsp_process/`:
```
flink_vision_job_proper.py      â† New PyFlink job
deploy_flink_vision_job.sh      â† Deployment script
```

### 2. Update in project root:
```
Dockerfile.flink                â† Updated with all dependencies
```

## Quick Deploy

### Step 1: Copy files

```bash
cd ~/smart-city-data-pipeline/pipline2

# Create the PyFlink job
nano data-processor/stream/rtsp_process/flink_vision_job_proper.py
# Paste the code from artifact

# Create deploy script
nano data-processor/stream/rtsp_process/deploy_flink_vision_job.sh
# Paste the deploy script

# Update Dockerfile
nano Dockerfile.flink
# Paste the updated Dockerfile

# Make executable
chmod +x data-processor/stream/rtsp_process/deploy_flink_vision_job.sh
```

### Step 2: Deploy

```bash
cd data-processor/stream/rtsp_process
./deploy_flink_vision_job.sh
```

This will:
1. Copy job to `flink-jobs/`
2. Rebuild Flink containers with Python
3. Restart Flink
4. Submit the job

### Step 3: Check Flink UI

```
http://localhost:8083
```

You should see: **"Vision Processor - Object Detection & Tracking"**

## Flink Pipeline Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Source                        â”‚
â”‚ Topic: smart-city-camera-frames     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YOLO Frame Processor (MapFunction)  â”‚
â”‚ - Decode base64                     â”‚
â”‚ - Run YOLO detection                â”‚
â”‚ - Extract age/gender                â”‚
â”‚ Parallelism: 2 (distributable)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Filter Invalid Frames               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Key By camera_id                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Object Tracker (KeyedProcessFunc)   â”‚
â”‚ - Stateful per camera               â”‚
â”‚ - Flink manages state               â”‚
â”‚ - Fault tolerant                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Result Printer (MapFunction)        â”‚
â”‚ - Print to console                  â”‚
â”‚ - TODO: Write to HDFS/TimescaleDB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Console Output                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Features

### 1. Distributed Processing
```python
env.set_parallelism(2)  # Run 2 parallel tasks
```

Flink automatically distributes work across TaskManagers!

### 2. Fault Tolerance
```python
env.enable_checkpointing(60000)  # Checkpoint every 60s
```

If a task fails, Flink restarts from last checkpoint!

### 3. Managed State
```python
class ObjectTracker(KeyedProcessFunction):
    def open(self, runtime_context):
        state_descriptor = ValueStateDescriptor(...)
        self.tracker_state = runtime_context.get_state(state_descriptor)
```

Flink manages tracker state per camera with fault tolerance!

## Monitoring

### Check if Job is Running

```bash
# Via Flink UI
http://localhost:8083

# Via command line
docker exec flink-jobmanager pgrep -f flink_vision_job_proper.py
```

### View Logs

```bash
# Job logs
docker exec flink-jobmanager tail -f /tmp/vision_job.log

# Flink logs
docker logs flink-jobmanager
docker logs flink-taskmanager
```

### Stop Job

```bash
docker exec flink-jobmanager pkill -f flink_vision_job_proper.py
```

## Differences from Old Code

| Feature | Old (kafka-python) | New (PyFlink) |
|---------|-------------------|---------------|
| Shows in UI | âŒ No | âœ… Yes |
| Uses Flink engine | âŒ No | âœ… Yes |
| Parallelization | âŒ Manual | âœ… Automatic |
| Fault tolerance | âŒ None | âœ… Checkpointing |
| State management | âŒ In-memory only | âœ… Managed by Flink |
| Monitoring | âŒ Manual | âœ… Flink UI |
| Scalability | âŒ Single process | âœ… Distributed |

## Adding Storage

### Write to HDFS

In `ResultPrinter.map()`:

```python
def map(self, value):
    data = json.loads(value)
    
    # Write to HDFS
    from hdfs3 import HDFileSystem
    hdfs = HDFileSystem(host='namenode', port=8020)
    
    date = data['timestamp'][:10]
    path = f"/smart-city/detections/{data['camera_id']}/{date}/"
    hdfs.mkdir(path, create_parents=True)
    
    filename = f"{path}/{data['timestamp']}.json"
    hdfs.write(filename, json.dumps(data).encode())
    
    return value
```

### Write to TimescaleDB

```python
def map(self, value):
    data = json.loads(value)
    
    # Write to TimescaleDB
    import psycopg2
    conn = psycopg2.connect(
        host='timescaledb',
        port=5432,
        database='realtime_analytics',
        user='smartcity',
        password='smartcity123'
    )
    
    cursor = conn.cursor()
    for det in data['detections']:
        cursor.execute("""
            INSERT INTO detections (...)
            VALUES (...)
        """)
    conn.commit()
    conn.close()
    
    return value
```

## Troubleshooting

### Job doesn't show in UI

Check logs:
```bash
docker exec flink-jobmanager cat /tmp/vision_job.log
```

Common issues:
- PyFlink not installed â†’ Check Dockerfile
- Import errors â†’ Rebuild containers
- Kafka not running â†’ Start Kafka

### Task failures

Check TaskManager logs:
```bash
docker logs flink-taskmanager
```

Common issues:
- Out of memory â†’ Increase TaskManager memory
- YOLO model not found â†’ Check model download in Dockerfile

### Performance issues

Adjust parallelism:
```python
env.set_parallelism(4)  # Increase parallel tasks
```

## Summary

âœ… **Proper PyFlink job** that uses Flink's engine  
âœ… **Shows in Flink UI** for monitoring  
âœ… **Distributed processing** across TaskManagers  
âœ… **Fault tolerant** with checkpointing  
âœ… **Managed state** per camera  
âœ… **Production-ready** architecture  

This is the RIGHT way to use Flink for stream processing! ğŸš€