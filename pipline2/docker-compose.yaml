# ============================================
# SMART CITY DATA PIPELINE - WITH HIVE
# Hadoop replaced with Hive for SQL analytics
# ============================================

networks:
  smart-city-net:
    driver: bridge

volumes:
  namenode_data:
  datanode_data:
  postgres_data:
  hive_metastore_data:
  # redis_data:
  # airflow_logs:
  spark_logs:
  flink_checkpoints:
  flink_savepoints:

services:
  # ============================================
  # ZOOKEEPER - Required for Kafka
  # ============================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    networks:
      - smart-city-net
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================
  # KAFKA BROKER - Data Ingestion
  # ============================================
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    networks:
      - smart-city-net
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 168
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5

  # ============================================
  # HADOOP NAMENODE - Data Lake (KEEP YOUR WORKING 3.3.6!)
  # ============================================
  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode
    hostname: namenode
    user: root
    networks:
      - smart-city-net
    ports:
      - "9870:9870"  # Web UI
      - "8020:8020"  # HDFS RPC port
    environment:
      - ENSURE_NAMENODE_DIR=/opt/hadoop/dfs/name
      - CLUSTER_NAME=smartcity
    env_file:
      - ./hadoop.env
    command: >
      bash -c "
      mkdir -p /opt/hadoop/dfs/name &&
      chmod -R 755 /opt/hadoop/dfs/name &&
      if [ ! -f /opt/hadoop/dfs/name/current/VERSION ]; then
        hdfs namenode -format -force -clusterID smartcity;
      fi &&
      hdfs namenode
      "
    volumes:
      - namenode_data:/opt/hadoop/dfs/name
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============================================
  # HADOOP DATANODE - Data Lake (KEEP YOUR WORKING 3.3.6!)
  # ============================================
  datanode:
    image: apache/hadoop:3.3.6
    container_name: datanode
    hostname: datanode
    user: root
    networks:
      - smart-city-net
    ports:
      - "9864:9864"
    env_file:
      - ./hadoop.env
    command: >
      bash -c "
      mkdir -p /opt/hadoop/dfs/data &&
      chmod -R 755 /opt/hadoop/dfs/data &&
      hdfs datanode
      "
    volumes:
      - datanode_data:/opt/hadoop/dfs/data
    depends_on:
      namenode:
        condition: service_healthy

  # ============================================
  # HIVE METASTORE POSTGRES - Metadata DB
  # ============================================
  hive-metastore-postgres:
    image: postgres:15-alpine
    container_name: hive-metastore-postgres
    networks:
      - smart-city-net
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
    volumes:
      - hive_metastore_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================
  # HIVE METASTORE - Metadata Service (works with Hadoop 3.3.6)
  # ============================================
  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    hostname: hive-metastore
    networks:
      - smart-city-net
    ports:
      - "9083:9083"
    environment:
      SERVICE_NAME: metastore
      # These variables are recognized by the 4.0.0 entrypoint to auto-init
      DB_DRIVER: postgres
      SERVICE_OPTS: "-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://hive-metastore-postgres:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive"
    # We remove the manual 'command' and let the entrypoint handle it
    # But we keep the healthcheck/depends_on to ensure DB is ready
    depends_on:
      hive-metastore-postgres:
        condition: service_healthy
      namenode:
        condition: service_healthy
    volumes:
      - ./hadoop.env:/etc/hadoop/hadoop.env:ro
      - ./postgresql-42.7.2.jar:/opt/hive/lib/postgresql-42.7.2.jar:ro # <--- ADD THIS

  # ============================================
  # HIVE SERVER - SQL Interface (works with Hadoop 3.3.6)
  # ============================================
  hiveserver2:
    image: apache/hive:4.0.0
    container_name: hiveserver2
    hostname: hiveserver2
    networks:
      - smart-city-net
    ports:
      - "10000:10000"
      - "10002:10002"
    environment:
      SERVICE_NAME: hiveserver2
      # Tell HiveServer2 where to find the metastore
      SERVICE_OPTS: "-Dhive.metastore.uris=thrift://hive-metastore:9083"
    depends_on:
      - hive-metastore
    volumes:
      - ./postgresql-42.7.2.jar:/opt/hive/lib/postgresql-42.7.2.jar:ro # <--- ADD THIS

  # ============================================
  # SPARK MASTER - Processing Engine
  # ============================================
  spark-master:
    image: apache/spark:3.5.3-java17-python3
    container_name: spark-master
    networks:
      - smart-city-net
    ports:
      - "8080:8080"  # Web UI
      - "7077:7077"  # Spark Master port
      - "4040:4040"  # Spark Application UI
      - "6066:6066"  # REST API port
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    command: >
      bash -c "
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master \
        --host spark-master \
        --port 7077 \
        --webui-port 8080 \
        --rest
      "
    volumes:
      - ./spark-jobs:/opt/spark-jobs
      - spark_logs:/opt/spark/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============================================
  # SPARK WORKER - Processing Engine
  # ============================================
  spark-worker:
    image: apache/spark:3.5.3-java17-python3
    container_name: spark-worker
    networks:
      - smart-city-net
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_WEBUI_PORT=8081
    command: >
      bash -c "
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
      "
    volumes:
      - ./spark-jobs:/opt/spark-jobs
    depends_on:
      - spark-master

  # ============================================
  # POSTGRESQL - Analytics Database
  # ============================================
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    networks:
      - smart-city-net
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: smartcity
      POSTGRES_PASSWORD: smartcity123
      POSTGRES_DB: smart_city_analytics
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U smartcity"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================
  # AIRFLOW POSTGRES - Metadata DB (MUST BE UNCOMMENTED)
  # ============================================
  airflow-postgres:
    image: postgres:15-alpine
    container_name: airflow-postgres
    networks:
      - smart-city-net
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./airflow-db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================
  # AIRFLOW WEBSERVER
  # ============================================
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-webserver
    networks:
      - smart-city-net
    ports:
      - "8082:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - SPARK_HOME=/opt/spark
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./spark-jobs:/opt/spark-jobs
      - ./hadoop.env:/etc/hadoop/hadoop.env:ro
    depends_on:
      airflow-postgres:
        condition: service_healthy
      spark-master:
        condition: service_healthy

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-scheduler
    networks:
      - smart-city-net
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      # IMPORTANT: Scheduler also needs Spark/Java paths to trigger jobs
      - SPARK_HOME=/opt/spark
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./spark-jobs:/opt/spark-jobs
      - ./hadoop.env:/etc/hadoop/hadoop.env:ro
    command: scheduler
    depends_on:
      airflow-postgres:
        condition: service_healthy

  # ============================================
  # FLINK JOBMANAGER - Stream Processing
  # ============================================
  flink-jobmanager:
    build:
      context: .
      dockerfile: Dockerfile.flink
    container_name: flink-jobmanager
    hostname: flink-jobmanager
    networks:
      - smart-city-net
    ports:
      - "8083:8081"
      - "6123:6123"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
        parallelism.default: 2
    volumes:
      - ./flink-jobs:/opt/flink-jobs
      - flink_checkpoints:/opt/flink/checkpoints
    command: jobmanager
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============================================
  # FLINK TASKMANAGER - Stream Processing
  # ============================================
  flink-taskmanager:
    build:
      context: .
      dockerfile: Dockerfile.flink
    container_name: flink-taskmanager
    networks:
      - smart-city-net
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
    volumes:
      - ./flink-jobs:/opt/flink-jobs
    command: taskmanager
    depends_on:
      flink-jobmanager:
        condition: service_healthy
  
  # ============================================
  # TIMESCALEDB - Time-Series Database
  # ============================================
  timescaledb:
    image: timescale/timescaledb:latest-pg15
    container_name: timescaledb
    networks:
      - smart-city-net
    ports:
      - "5433:5432"
    environment:
      POSTGRES_USER: smartcity
      POSTGRES_PASSWORD: smartcity123
      POSTGRES_DB: realtime_analytics
    volumes:
      - ./init-timescaledb.sql:/docker-entrypoint-initdb.d/init.sql

  # ============================================
  # GRAFANA - Visualization
  # ============================================
  grafana:
    image: grafana/grafana:10.2.3
    container_name: grafana
    networks:
      - smart-city-net
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - postgres
      - timescaledb
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3